{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1B7_5ukPQHGrg89xd-jeBjofOuyf-e-HX","timestamp":1768673769581}],"gpuType":"T4","runtime_attributes":{"runtime_version":"2025.10"},"mount_file_id":"1B7_5ukPQHGrg89xd-jeBjofOuyf-e-HX","authorship_tag":"ABX9TyNCaCZ/Kag5nLMN+EUXDeG3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-s5Pzf5Acusl","executionInfo":{"status":"ok","timestamp":1768670672681,"user_tz":-330,"elapsed":4352,"user":{"displayName":"Geeta. Gokavi","userId":"07691925561950042219"}},"outputId":"e0811b16-709d-4202-8a5a-21fd492a9d5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","True\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["!pip install rouge-score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WY3Cljnc3pJk","executionInfo":{"status":"ok","timestamp":1768672973243,"user_tz":-330,"elapsed":40363,"user":{"displayName":"Geeta. Gokavi","userId":"07691925561950042219"}},"outputId":"041a7de0-f4c3-4625-d011-2713c4c028b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=40d9a3caed4ef37cba7acb04b7ee0c40d6fdaaa0a1cdffccc80c11bb4a6510f5\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n"]}]},{"cell_type":"markdown","source":["model tokenization"],"metadata":{"id":"sZlVKT_aPLG3"}},{"cell_type":"code","source":["# ===============================\n","# Imports\n","# ===============================\n","import pandas as pd\n","import re\n","import html\n","import warnings\n","import os\n","import torch\n","\n","from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n","from transformers import AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","# ===============================\n","# Suppress warnings\n","# ===============================\n","warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n","\n","# ===============================\n","# Paths\n","# ===============================\n","DATA_PATH = \"/content/drive/MyDrive/NewsSumm Dataset.xlsx\"\n","SAVE_DIR  = \"/content/drive/MyDrive/tokenized_flan_t5_small_data\"\n","\n","TRAIN_DIR = os.path.join(SAVE_DIR, \"train\")\n","TEST_DIR  = os.path.join(SAVE_DIR, \"test\")\n","\n","os.makedirs(TRAIN_DIR, exist_ok=True)\n","os.makedirs(TEST_DIR, exist_ok=True)\n","\n","# ===============================\n","# Load dataset\n","# ===============================\n","df = pd.read_excel(DATA_PATH)\n","df = df.dropna(subset=[\"article_text\", \"human_summary\"])\n","print(\"Rows after cleaning:\", len(df))\n","\n","# ===============================\n","# Resample to 15,000 rows\n","# ===============================\n","df = df.sample(n=15000, random_state=42)\n","print(\"Rows after resampling:\", len(df))\n","\n","# ===============================\n","# Clean text function\n","# ===============================\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return \"\"\n","\n","    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")\n","    text = html.unescape(text)\n","    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n","    text = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"\", text)\n","    text = re.sub(r\"\\+?\\d[\\d\\s\\-]{8,}\\d\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","\n","    return text\n","\n","df[\"article_text\"]  = df[\"article_text\"].apply(clean_text)\n","df[\"human_summary\"] = df[\"human_summary\"].apply(clean_text)\n","\n","# ===============================\n","# Train / Test split\n","# ===============================\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","print(\"Train rows:\", len(train_df))\n","print(\"Test rows:\", len(test_df))\n","\n","# ===============================\n","# Load FLAN-T5 SMALL tokenizer\n","# ===============================\n","tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n","\n","# ===============================\n","# Tokenization settings (SAFE)\n","# ===============================\n","BATCH_SIZE = 4          # ↓ from 8\n","MAX_INPUT_LEN = 384     # ↓ from 512\n","MAX_TARGET_LEN = 96     # ↓ from 128\n","\n","# ===============================\n","# Batch tokenization function\n","# ===============================\n","def tokenize_and_save(articles, summaries, out_dir, name):\n","\n","    batch_id = 0\n","\n","    for i in range(0, len(articles), BATCH_SIZE):\n","\n","        batch_articles = articles[i:i+BATCH_SIZE]\n","        batch_summaries = summaries[i:i+BATCH_SIZE]\n","\n","        enc = tokenizer(\n","            batch_articles,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=MAX_INPUT_LEN,\n","            return_tensors=\"pt\"\n","        )\n","\n","        dec = tokenizer(\n","            batch_summaries,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=MAX_TARGET_LEN,\n","            return_tensors=\"pt\"\n","        )\n","\n","        labels = dec[\"input_ids\"]\n","        labels[labels == tokenizer.pad_token_id] = -100  # ignore padding\n","\n","        batch_data = {\n","            \"input_ids\": enc[\"input_ids\"],\n","            \"attention_mask\": enc[\"attention_mask\"],\n","            \"labels\": labels\n","        }\n","\n","        save_path = os.path.join(out_dir, f\"batch_{batch_id}.pt\")\n","        torch.save(batch_data, save_path)\n","\n","        if batch_id % 50 == 0:\n","            print(f\"{name}: saved batch {batch_id}\")\n","\n","        batch_id += 1\n","\n","    print(f\"{name} tokenization done. Total batches: {batch_id}\")\n","\n","# ===============================\n","# Run tokenization\n","# ===============================\n","tokenize_and_save(\n","    train_df[\"article_text\"].astype(str).tolist(),\n","    train_df[\"human_summary\"].astype(str).tolist(),\n","    TRAIN_DIR,\n","    \"TRAIN\"\n",")\n","\n","tokenize_and_save(\n","    test_df[\"article_text\"].astype(str).tolist(),\n","    test_df[\"human_summary\"].astype(str).tolist(),\n","    TEST_DIR,\n","    \"TEST\"\n",")\n","\n","print(\"\\n✅ FLAN-T5 SMALL batch-wise tokenization completed successfully\")\n"],"metadata":{"id":"kgNvVaqQ8vAN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model training"],"metadata":{"id":"LN2TqWKPPUAI"}},{"cell_type":"code","source":["import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from torch.optim import AdamW\n","\n","# --------------------\n","# Paths\n","# --------------------\n","TRAIN_DIR = \"/content/drive/MyDrive/tokenized_flan_t5_small_data/train\"\n","SAVE_DIR  = \"/content/drive/MyDrive/flan_t5_small_finetune_trained\"\n","MODEL_NAME = \"google/flan-t5-small\"\n","\n","# --------------------\n","# Device\n","# --------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# --------------------\n","# Load model & tokenizer\n","# --------------------\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","model.to(device)\n","model.train()\n","model.config.pad_token_id = tokenizer.pad_token_id\n","\n","# --------------------\n","# Optimizer\n","# --------------------\n","optim = AdamW(model.parameters(), lr=2e-5)\n","\n","# --------------------\n","# Load batch files\n","# --------------------\n","files = [f for f in os.listdir(TRAIN_DIR) if f.endswith(\".pt\")]\n","files.sort()\n","files = files[:5000]   # safe large number for flan-t5-small\n","\n","print(\"Total batches:\", len(files))\n","\n","total_loss = 0.0\n","\n","# --------------------\n","# Training loop\n","# --------------------\n","for i, fname in enumerate(files, 1):\n","\n","    batch = torch.load(os.path.join(TRAIN_DIR, fname))\n","\n","    input_ids = batch[\"input_ids\"].to(device)\n","    attention_mask = batch[\"attention_mask\"].to(device)\n","    labels = batch[\"labels\"].to(device)\n","\n","    # safety (already -100, but keep)\n","    labels[labels == tokenizer.pad_token_id] = -100\n","\n","    optim.zero_grad()\n","\n","    outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        labels=labels\n","    )\n","\n","    loss = outputs.loss\n","    loss.backward()\n","    optim.step()\n","\n","    total_loss += loss.item()\n","\n","    if i % 100 == 0:\n","        print(f\"loss {i}: {total_loss / i:.4f}\")\n","\n","    # Free memory\n","    del batch, input_ids, attention_mask, labels, outputs, loss\n","    torch.cuda.empty_cache()\n","\n","# --------------------\n","# Save model\n","# --------------------\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","model.save_pretrained(SAVE_DIR)\n","tokenizer.save_pretrained(SAVE_DIR)\n","\n","print(\"FLAN-T5-SMALL fine-tuning completed\")\n"],"metadata":{"id":"vNxUNnW5DOz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model evaluation"],"metadata":{"id":"YeHx9s9NPtbb"}},{"cell_type":"code","source":["import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from rouge_score import rouge_scorer\n","\n","# --------------------\n","# Paths\n","# --------------------\n","TEST_DIR  = \"/content/drive/MyDrive/tokenized_flan_t5_small_data/test\"\n","MODEL_DIR = \"/content/drive/MyDrive/flan_t5_small_finetune_trained\"\n","\n","# --------------------\n","# Device\n","# --------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# --------------------\n","# Load model & tokenizer\n","# --------------------\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(device)\n","model.eval()\n","\n","# --------------------\n","# ROUGE scorer\n","# --------------------\n","scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n","\n","r1 = r2 = rl = 0.0\n","r11 = r21 = rl1 = 0.0\n","count = 0\n","\n","files = sorted([f for f in os.listdir(TEST_DIR) if f.endswith(\".pt\")])\n","\n","# --------------------\n","# Evaluation loop\n","# --------------------\n","for fname in files:\n","\n","    batch = torch.load(os.path.join(TEST_DIR, fname))\n","\n","    input_ids = batch[\"input_ids\"].to(device)\n","    attention_mask = batch[\"attention_mask\"].to(device)\n","\n","    labels = batch[\"labels\"].clone()\n","    labels[labels == -100] = tokenizer.pad_token_id   # restore padding for decoding\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            max_new_tokens=50\n","        )\n","\n","    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","    refs  = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    for pred, ref in zip(preds, refs):\n","        scores = scorer.score(ref, pred)\n","\n","        r1  += scores[\"rouge1\"].fmeasure\n","        r2  += scores[\"rouge2\"].fmeasure\n","        rl  += scores[\"rougeL\"].fmeasure\n","        r11 += scores[\"rouge1\"].precision\n","        r21 += scores[\"rouge2\"].precision\n","        rl1 += scores[\"rougeL\"].precision\n","\n","        count += 1\n","\n","# --------------------\n","# Final results\n","# --------------------\n","print(\"\\nROUGE RESULTS (FLAN-T5-SMALL)\")\n","print(\"ROUGE-1 F1:\", round(r1 / count, 4))\n","print(\"ROUGE-2 F1:\", round(r2 / count, 4))\n","print(\"ROUGE-L F1:\", round(rl / count, 4))\n","print(\"ROUGE-1 Precision:\", round(r11 / count, 4))\n","print(\"ROUGE-2 Precision:\", round(r21 / count, 4))\n","print(\"ROUGE-L Precision:\", round(rl1 / count, 4))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ToQfdHAHSTv","executionInfo":{"status":"ok","timestamp":1768673644542,"user_tz":-330,"elapsed":649148,"user":{"displayName":"Geeta. Gokavi","userId":"07691925561950042219"}},"outputId":"234664d8-e4a5-4180-da2f-b88f65827b1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","\n","ROUGE RESULTS (FLAN-T5-SMALL)\n","ROUGE-1 F1: 0.4492\n","ROUGE-2 F1: 0.2643\n","ROUGE-L F1: 0.358\n","ROUGE-1 Precision: 0.6152\n","ROUGE-2 Precision: 0.3655\n","ROUGE-L Precision: 0.4898\n"]}]}]}