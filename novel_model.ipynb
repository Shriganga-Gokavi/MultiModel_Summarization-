{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"runtime_attributes":{"runtime_version":"2025.10"},"authorship_tag":"ABX9TyMcxiUzwy3I295p+kyKK4fz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fsRoQQS4Whv3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","source":["tokenization"],"metadata":{"id":"iEBnVA_vYqie"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import html\n","import warnings\n","import os\n","import torch\n","\n","from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n","from transformers import AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","\n","# -------------------------------\n","# Suppress warnings\n","# -------------------------------\n","warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n","\n","# -------------------------------\n","# Paths\n","# -------------------------------\n","DATA_PATH = \"/content/drive/MyDrive/NewsSumm Dataset.xlsx\"\n","SAVE_DIR  = \"/content/drive/MyDrive/tokenized_mistral_batches1\"\n","\n","TRAIN_DIR = os.path.join(SAVE_DIR, \"train\")\n","TEST_DIR  = os.path.join(SAVE_DIR, \"test\")\n","\n","os.makedirs(TRAIN_DIR, exist_ok=True)\n","os.makedirs(TEST_DIR, exist_ok=True)\n","\n","# -------------------------------\n","# Load dataset\n","# -------------------------------\n","df = pd.read_excel(DATA_PATH)\n","\n","# CHANGE: include article_text\n","df = df.dropna(subset=[\"human_summary\", \"article_text\"])\n","print(\"Rows after cleaning:\", len(df))\n","\n","# -------------------------------\n","# Resample\n","# -------------------------------\n","df = df.sample(n=15000, random_state=42)\n","print(\"Rows after resampling:\", len(df))\n","\n","# -------------------------------\n","# Clean text\n","# -------------------------------\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")\n","    text = html.unescape(text)\n","    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n","    text = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"\", text)\n","    text = re.sub(r\"\\+?\\d[\\d\\s\\-]{8,}\\d\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","df[\"human_summary\"] = df[\"human_summary\"].apply(clean_text)\n","df[\"article_text\"]  = df[\"article_text\"].apply(clean_text)\n","\n","# -------------------------------\n","# Train / Test split\n","# -------------------------------\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# CHANGE: input = article, label = summary\n","train_texts = train_df[\"article_text\"].astype(str).tolist()\n","test_texts  = test_df[\"article_text\"].astype(str).tolist()\n","\n","train_labels = train_df[\"human_summary\"].astype(str).tolist()\n","test_labels  = test_df[\"human_summary\"].astype(str).tolist()\n","\n","print(\"Train rows:\", len(train_texts))\n","print(\"Test rows:\", len(test_texts))\n","\n","# -------------------------------\n","# Load Mistral tokenizer\n","# -------------------------------\n","MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    MODEL_NAME,\n","    use_fast=True,\n","    trust_remote_code=True\n",")\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","tokenizer.padding_side = \"right\"\n","\n","# -------------------------------\n","# Tokenization settings (SAFE)\n","# -------------------------------\n","BATCH_SIZE = 4\n","MAX_LEN = 128\n","\n","# -------------------------------\n","# Batch tokenization function\n","# -------------------------------\n","def tokenize_and_save(inputs, labels, out_dir, split_name):\n","\n","    print(f\"\\nTokenizing {split_name} set...\")\n","\n","    batch_id = 0\n","\n","    for i in range(0, len(inputs), BATCH_SIZE):\n","\n","        batch_inputs = inputs[i:i+BATCH_SIZE]\n","        batch_labels = labels[i:i+BATCH_SIZE]\n","\n","        enc_inputs = tokenizer(\n","            batch_inputs,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=MAX_LEN,\n","            return_tensors=\"pt\"\n","        )\n","\n","        enc_labels = tokenizer(\n","            batch_labels,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=MAX_LEN,\n","            return_tensors=\"pt\"\n","        )\n","\n","        batch_data = {\n","            \"input_ids\": enc_inputs[\"input_ids\"],\n","            \"attention_mask\": enc_inputs[\"attention_mask\"],\n","            \"labels\": enc_labels[\"input_ids\"]\n","        }\n","\n","        save_path = os.path.join(out_dir, f\"batch_{batch_id}.pt\")\n","        torch.save(batch_data, save_path)\n","\n","        if batch_id % 50 == 0:\n","            print(f\"{split_name}: saved batch {batch_id}\")\n","\n","        batch_id += 1\n","\n","    print(f\"{split_name} tokenization done. Total batches: {batch_id}\")\n","\n","# -------------------------------\n","# Run tokenization\n","# -------------------------------\n","tokenize_and_save(train_texts, train_labels, TRAIN_DIR, \"TRAIN\")\n","tokenize_and_save(test_texts, test_labels, TEST_DIR, \"TEST\")\n","\n","print(\"\\nMistral-7B article→summary batch-wise tokenization completed successfully\")\n"],"metadata":{"id":"PpEfPHkPWi9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q bitsandbytes accelerate peft transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3dw4KDUfGUG","executionInfo":{"status":"ok","timestamp":1769260148183,"user_tz":-330,"elapsed":19399,"user":{"displayName":"1065 Shriganga A G","userId":"11943577731829294603"}},"outputId":"acc58dde-d88f-471d-ddda-dc50c0f24d23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["model_training"],"metadata":{"id":"YMbMgALsYuhG"}},{"cell_type":"code","source":["import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from torch.optim import AdamW\n","from peft import LoraConfig, get_peft_model\n","\n","# --------------------\n","# Paths\n","# --------------------\n","TRAIN_DIR = \"/content/drive/MyDrive/tokenized_mistral_batches1/train/\"\n","SAVE_DIR  = \"/content/drive/MyDrive/mistral_qlora_finetuned1\"\n","MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n","\n","# --------------------\n","# Device\n","# --------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# --------------------\n","# Tokenizer\n","# --------------------\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# --------------------\n","# QLoRA config\n","# --------------------\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\"\n",")\n","\n","# --------------------\n","# Load model\n","# --------------------\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\"\n",")\n","\n","# ✅ keep this (fixes grad error)\n","model.enable_input_require_grads()\n","\n","\n","model.config.use_cache = False\n","\n","# --------------------\n","# LoRA config\n","# --------------------\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.train()\n","\n","# --------------------\n","# Optimizer\n","# --------------------\n","optimizer = AdamW(model.parameters(), lr=2e-4)\n","\n","# --------------------\n","# Load batches\n","# --------------------\n","files = sorted([f for f in os.listdir(TRAIN_DIR) if f.endswith(\".pt\")])\n","print(\"Total batches:\", len(files))\n","\n","total_loss = 0\n","\n","# --------------------\n","# Training loop\n","# --------------------\n","for step, fname in enumerate(files, 1):\n","\n","    batch = torch.load(os.path.join(TRAIN_DIR, fname))\n","\n","    input_ids = batch[\"input_ids\"].to(device)\n","    attention_mask = batch[\"attention_mask\"].to(device)\n","    labels = batch[\"labels\"].to(device)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        labels=labels\n","    )\n","\n","    loss = outputs.loss\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    total_loss += loss.item()\n","\n","    if step % 50 == 0:\n","        print(f\"Step {step} | Avg Loss: {total_loss / step:.4f}\")\n","\n","    del batch, input_ids, attention_mask, labels, outputs, loss\n","    torch.cuda.empty_cache()\n","\n","# --------------------\n","# Save adapters\n","# --------------------\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","model.save_pretrained(SAVE_DIR)\n","tokenizer.save_pretrained(SAVE_DIR)\n","\n","print(\"✅ QLoRA Mistral training completed successfully (hybrid fast mode)\")\n"],"metadata":{"id":"EDsUH00Pl06_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["model evaluation"],"metadata":{"id":"h8tVVLBeYydK"}},{"cell_type":"code","source":["!pip install rouge-score\n"],"metadata":{"id":"VffaBIk3_eDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from peft import PeftModel\n","from rouge_score import rouge_scorer\n","\n","# --------------------\n","# Paths\n","# --------------------\n","TEST_DIR  = \"/content/drive/MyDrive/tokenized_mistral_batches1/test/\"\n","ADAPTER_DIR = \"/content/drive/MyDrive/mistral_qlora_finetuned1\"\n","BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n","\n","# --------------------\n","# Device\n","# --------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# --------------------\n","# Tokenizer  (FIXED)\n","# --------------------\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"   # ✅ MUST be right\n","\n","# --------------------\n","# QLoRA config\n","# --------------------\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\"\n",")\n","\n","# --------------------\n","# Load base model\n","# --------------------\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\"\n",")\n","\n","# --------------------\n","# Load adapters\n","# --------------------\n","model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n","model.eval()\n","\n","# --------------------\n","# ROUGE\n","# --------------------\n","scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n","\n","r1 = r2 = rl = 0.0\n","r1p = r2p = rlp = 0.0\n","count = 0\n","\n","files = sorted([f for f in os.listdir(TEST_DIR) if f.endswith(\".pt\")])\n","print(\"Test batches:\", len(files))\n","\n","# --------------------\n","# Evaluation loop\n","# --------------------\n","for fname in files:\n","\n","    batch = torch.load(os.path.join(TEST_DIR, fname))\n","\n","    input_ids = batch[\"input_ids\"].to(device)\n","    attention_mask = batch[\"attention_mask\"].to(device)\n","\n","    # ✅ FIX labels\n","    labels = batch[\"labels\"].clone()\n","    labels[labels == -100] = tokenizer.pad_token_id\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            max_new_tokens=120,   # ✅ longer\n","            do_sample=False,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    # remove prompt\n","    gen_only = outputs\n","\n","\n","    preds = tokenizer.batch_decode(gen_only, skip_special_tokens=True)\n","    refs  = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    for pred, ref in zip(preds, refs):\n","        scores = scorer.score(ref, pred)\n","\n","        r1  += scores[\"rouge1\"].fmeasure\n","        r2  += scores[\"rouge2\"].fmeasure\n","        rl  += scores[\"rougeL\"].fmeasure\n","\n","        r1p += scores[\"rouge1\"].precision\n","        r2p += scores[\"rouge2\"].precision\n","        rlp += scores[\"rougeL\"].precision\n","\n","        count += 1\n","\n","# --------------------\n","# Results\n","# --------------------\n","print(\"\\nROUGE RESULTS (Mistral QLoRA)\")\n","print(\"ROUGE-1 F1:\", round(r1 / count, 4))\n","print(\"ROUGE-2 F1:\", round(r2 / count, 4))\n","print(\"ROUGE-L F1:\", round(rl / count, 4))\n","print(\"ROUGE-1 Precision:\", round(r1p / count, 4))\n","print(\"ROUGE-2 Precision:\", round(r2p / count, 4))\n","print(\"ROUGE-L Precision:\", round(rlp / count, 4))\n"],"metadata":{"id":"8Kra4gHS_OsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Data\n","df = pd.DataFrame({\n","    \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n","    \"Score\": [0.1137, 0.0271, 0.0749, 0.5254, 0.3050, 0.3951],\n","    \"Model\": [\"Summary-only\"]*3 + [\"Article and Summary\"]*3\n","})\n","\n","# Plot\n","sns.barplot(data=df, x=\"Metric\", y=\"Score\", hue=\"Model\")\n","plt.title(\"ROUGE Comparison\")\n","plt.show()\n"],"metadata":{"id":"SD0Od45QVjIq"},"execution_count":null,"outputs":[]}]}